{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "- [dominhhai rnn là gì](https://dominhhai.github.io/vi/2017/10/what-is-rnn/)\n",
    "- [nttuan rnn](https://nttuan8.com/bai-13-recurrent-neural-network/)\n",
    "- https://forum.machinelearningcoban.com/t/recurrent-neural-network-hoat-dong-the-nao/3040/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "- Về cơ bản mạng RNN là vòng lặp của NN\n",
    "- Sử dụng output của lần t-1 để tiếp tục train cho t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [RNN - Recurrent neural networks](https://www.coursera.org/learn/ai/lecture/drWZy/recurrent-neural-networks)\n",
    "- https://blogs.nvidia.com/blog/2019/05/22/drive-labs-predicting-future-motion/?ncid=so-you-t7-90294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "- [lstm 1997](https://dominhhai.github.io/vi/2017/10/what-is-lstm/)\n",
    "- [lstm_nntuan8](https://nttuan8.com/bai-14-long-short-term-memory-lstm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/lstm1.png\" width=600 height =600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Di chuyển từ cell C_t-1 sang cell C_t\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [LSTM](https://www.coursera.org/learn/ai/lecture/osoOq/lstms)\n",
    "- [LSTM is dead. Long Live Transformers]\n",
    "\t- Một video đầy đủ nói về LSTM/Transformers/BERT, [https://youtu.be/S27pHKBEp30](https://youtu.be/S27pHKBEp30?fbclid=IwAR0o8BA2aLex09NhXVNXeIVNeUYa1jFNIWVYxLGsjUS1cuNJOamk2H3AM6g)\n",
    "\t- Với dữ liệu dạng sequence (time series) như đoạn văn, video mọi người hay nghe thấy mô hình Recurrent Neural Network (RNN). Tuy nhiên mô hình RNN truyền thống bị vanishing gradient nên học không được tốt. Về sau có mô hình Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU) đỡ vanishing gradient hơn RNN truyền thống và thường xuyên được sử dụng. Từ đó khi nói về bài toán sử dụng model RNN thì ngầm hiểu là dùng LSTM/GRU chứ ít ai dùng RNN thuần nữa.\n",
    "\t- Giống như Resnet (2015) thì Attention (2017) cũng là một đột phá trong deep learning. Mô hình Transformer được dựng lên từ Seq2seq cho bài toán dịch với cơ chế attention. Mọi người nghe thấy nhiều về Bidirectional Encoder Representations from Transformers (BERT) hay PhoBERT cho tiếng việt, do BERT là một pre-trained model tốt cho nhiều task liên quan đến Natural Language Processing (NLP). BERT còn rất tiện cho mọi người làm các task về NLP và có performance tốt hơn hẳn RNN/LSTM. Tuy nhiên, thực ra BERT chính là phần Encoder của Transformer.\n",
    "\t- Transformer hay BERT là công nghệ mới, LSTM cũ nhưng không hẳn là đã chết, ví dụ LSTM vẫn tốt khi chiều dài chuỗi quá dài, ví Transformer cần O(N^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ứng Dụng\n",
    "[implement an LSTM Auto-encoder based Anomaly Detector in Keras](https://www.coursera.org/learn/ai/lecture/KHQ8y/how-to-implement-an-anomaly-detector-1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
