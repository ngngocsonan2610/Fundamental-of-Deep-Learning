{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "- [dominhhai rnn là gì](https://dominhhai.github.io/vi/2017/10/what-is-rnn/)\n",
    "- [nttuan rnn](https://nttuan8.com/bai-13-recurrent-neural-network/)\n",
    "\n",
    "More:\n",
    "- [MEDIUM- Lí Thuyết RNN & Các dạng của RNN](https://medium.com/datadriveninvestor/recurrent-neural-networks-in-deep-learning-part2-ce9fe1770a31)\n",
    "- [Ngọc_VietAI - giải thích RNN trong Seq2Seq ứng dụng NLP](https://www.youtube.com/watch?v=t0EoeTYU-fc&list=PLMm4sOMuA2QI5x_0KlNT3LuKDi6-ByboB&index=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "- Về cơ bản mạng RNN là vòng lặp của NN\n",
    "- Sử dụng output của lần t-1 để tiếp tục train cho t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ứng dụng trong NLP [AdNLP_VIETAI](https://www.youtube.com/watch?v=t0EoeTYU-fc&list=PLMm4sOMuA2QI5x_0KlNT3LuKDi6-ByboB&index=21):\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1-C3sy6ecv2cm3-K_kn1nU6Q9p3xGiko5' width=600 height = 600>\n",
    "\n",
    "Nếu có 3 từ liên tiếp nhau sẽ có 3 vector\n",
    "- xt-1 = tôi với vector học $h_{t-1}$, sau khi có từ tại t-1 -> thì sẽ out ra tất cả các từ có thể cùng xác suất, cao nhất là \"đi\"\n",
    "- h_t sẽ lưu thông tin của h_t-1, h_t sẽ có thông tin của \"tôi + đi\", và predict xác suất các từ tiếp theo\n",
    "- x_t+1 sẽ có  thông tin là \"tôi di học\" và tiếp tục predict ra là \"AI\"\n",
    "\n",
    "các vector $h_t$ sẽ được train với nhiều điểm dữ liệu \n",
    "- dùng chung 1 $W$ cho các bước"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN node:\n",
    "<img src='https://drive.google.com/uc?id=1z_kyuT6P--oeF-w8iTnACOZ0yCMZOq_w' width=600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô phòng:\n",
    "- $h_t$ chính là mô phỏng lịch sử thông tin -> những research mới sẽ cố improve h_t, \n",
    "    - ví dụ h_t hiện tại là phép cộng, có thể thử với phép nhân và các công thức khác\n",
    "    - mạng LSTM và GRU cũng chị optimize hơn ở việc xác định công thức $h_t$\n",
    "- $h_0$ có thể là vector 0\n",
    "- $\\hat{y}$ là xác suất trên toàn bộ tập từ điển\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pros:\n",
    "- chỉ có 1 $W$ có thể dùng cho toàn bộ độ dài của câu (model size không phụ thuộc vào độ dài câu)\n",
    "- sử dụng sigmoid activation func, RNN dễ bị vanishing gradients\n",
    "cons:\n",
    "- phụ thuộc vào nhau, nên training rất lâu\n",
    "- nếu xài phép nhân -> khi quá nhiều W sẽ bị về 0\n",
    "- [How LSTM networks solve the problem of vanishing gradients](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research & Implementation:\n",
    "- [Sử dụng RNN trong CV, để dự đoán hướng di chuyển của object](https://blogs.nvidia.com/blog/2019/05/22/drive-labs-predicting-future-motion/?ncid=so-you-t7-90294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các Loại RNN\n",
    "<img src='https://miro.medium.com/max/1204/0*EQIzoFoF-8FcjYWs' width=600 height = 600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "- [lstm 1997](https://dominhhai.github.io/vi/2017/10/what-is-lstm/)\n",
    "- [lstm_nntuan8](https://nttuan8.com/bai-14-long-short-term-memory-lstm/)\n",
    "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/lstm1.png\" width=600 height =600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Di chuyển từ cell C_t-1 sang cell C_t\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [LSTM](https://www.coursera.org/learn/ai/lecture/osoOq/lstms)\n",
    "- [LSTM is dead. Long Live Transformers]\n",
    "\t- Một video đầy đủ nói về LSTM/Transformers/BERT, [https://youtu.be/S27pHKBEp30](https://youtu.be/S27pHKBEp30?fbclid=IwAR0o8BA2aLex09NhXVNXeIVNeUYa1jFNIWVYxLGsjUS1cuNJOamk2H3AM6g)\n",
    "\t- Với dữ liệu dạng sequence (time series) như đoạn văn, video mọi người hay nghe thấy mô hình Recurrent Neural Network (RNN). Tuy nhiên mô hình RNN truyền thống bị vanishing gradient nên học không được tốt. Về sau có mô hình Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU) đỡ vanishing gradient hơn RNN truyền thống và thường xuyên được sử dụng. Từ đó khi nói về bài toán sử dụng model RNN thì ngầm hiểu là dùng LSTM/GRU chứ ít ai dùng RNN thuần nữa.\n",
    "\t- Giống như Resnet (2015) thì Attention (2017) cũng là một đột phá trong deep learning. Mô hình Transformer được dựng lên từ Seq2seq cho bài toán dịch với cơ chế attention. Mọi người nghe thấy nhiều về Bidirectional Encoder Representations from Transformers (BERT) hay PhoBERT cho tiếng việt, do BERT là một pre-trained model tốt cho nhiều task liên quan đến Natural Language Processing (NLP). BERT còn rất tiện cho mọi người làm các task về NLP và có performance tốt hơn hẳn RNN/LSTM. Tuy nhiên, thực ra BERT chính là phần Encoder của Transformer.\n",
    "\t- Transformer hay BERT là công nghệ mới, LSTM cũ nhưng không hẳn là đã chết, ví dụ LSTM vẫn tốt khi chiều dài chuỗi quá dài, ví Transformer cần O(N^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ứng Dụng\n",
    "[implement an LSTM Auto-encoder based Anomaly Detector in Keras](https://www.coursera.org/learn/ai/lecture/KHQ8y/how-to-implement-an-anomaly-detector-1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
