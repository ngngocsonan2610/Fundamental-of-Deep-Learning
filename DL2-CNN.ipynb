{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Neural Network\n",
    "Table of contents:\n",
    "- [VGG](#VGG)\n",
    "- [Googlenet-Inception](#Googlenet-Inception)\n",
    "- [Resnet](#ResNet)\n",
    "- [Densenet](#Densenet)\n",
    "- [Mobilenet](#Mobilenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- [Các cấu trúc phổ biến của mạng CNN](https://forum.machinelearningcoban.com/t/kien-truc-cac-mang-cnn-noi-tieng-phan-1-alex-lenet-inception-vgg/2582)\n",
    "- [Qua trinh phat trien NN](https://dlapplications.github.io/2018-07-06-CNN/)\n",
    "- [more comparison explain 1](https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "- [more comparison explain 2](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e4b1)\n",
    "- [more comparison explain 3](https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tính para\n",
    "<img src='https://drive.google.com/uc?id=10esJOPK_FMUdUCOePcOwtqQ82l2j1id-' width=600 height = 600>\n",
    "\n",
    "\n",
    "Lưu ý:\n",
    "- Output của convolutional layer sẽ qua hàm activation function trước khi trở thành input của convolutional layer tiếp theo.\n",
    "- Tổng số parameter của layer: Mỗi kernel có kích thước F*F*D và có 1 hệ số bias, nên tổng parameter của 1 kernel là F * F * D + 1. Mà convolutional layer áp dụng K kernel => Tổng số parameter trong layer này là K * (F * F * D + 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/conv.gif?resize=1000%2C562&ssl=1\" width=400 height = 400>\n",
    "\n",
    "Convolutional neural network là một mạng neural được ứng dụng rất nhiều trong deep learning trong computer vision cho classifier và localizer . \n",
    "Từ mạng CNN cơ bản người ta có thể tạo ra rất nhiều architect khác nhau, từ những mạng neural cơ bản 1 đến 2 layer đến 100 layer. \n",
    "Đã bao giờ bạn tự hỏi:\n",
    "   - nên sử dụng bao nhiêu layer, nên kết hợp conv với maxpooling thế nào? \n",
    "   - conv-maxpooling hay conv-conv-maxplooling ? \n",
    "   - hay nên sử dụng kernel 3x3 hay 5x5 thậm chí 7x7 điểm khác biệt là gì ? \n",
    "   - Làm gì khi model bị vanishing/exploding gradient, hay tại sao thi thêm nhiều layer hơn thì theo lý thuyết accuarcy phải cao hơn so với shallow model, nhưng thực tế lại không phải accuarcy không tăng thậm chí là giảm đó có phải nguyên nhân do overfitting.\n",
    "Việc tìm hiểu các architure nổi tiếng để xem cấu trúc của nó như thế nào, các ý tưởng về CNN mới nhất hiện nay, từ đó ta có thể trả lời được mấy câu hỏi trên. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "- https://dlapplications.github.io/2018-07-06-CNN/\n",
    "- http://localhost:8888/lab/tree/Git/DL/0.MsDSKHTN_DL_%C4%90%C4%83ng/MayHocNangCao-19-20-HK1/Lec_01_CNN_Architectures.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5\n",
    "parameter ở các lớp fully connected lớn hơn CNN rất nhiều\n",
    "> Đối với Lenet-5, thì số parameter ở các lớp fully connected chiểm >58k/60k para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source hay:\n",
    "- https://nttuan8.com/bai-6-convolutional-neural-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LeNet(1998)** \n",
    "    - Cấu trúc của LeNet gồm 2 layer (Convolution + maxpooling) và 2 layer fully connected layer và output là softmax layer \n",
    "    - Softmax layer, output = 10 (train với mnist nên 10 digits). Nhược điểm của LeNet là mạng còn rất đơn giản và sử dụng sigmoid (or tanh) ở mỗi convolution layer mạng tính toán rất chậm.\n",
    "- **Alexnet(2012)**\n",
    "    - Sử dụng relu thay cho sigmoid(or tanh) để xử lý với non-linearity. Tăng tốc độ tính toán lên 6 lần.\n",
    "    - Sử dụng dropout như một phương pháp regularization mới cho CNN. Dropout không những giúp mô hình tránh được overfitting mà còn làm giảm thời gian huấn luyện mô hình\n",
    "    - Overlap pooling để giảm size của network ( Traditionally pooling regions không overlap).\n",
    "    - Sử dụng local response normalization để chuẩn hóa ở mỗi layer.\n",
    "    - Sử dụng kỹ thuật data augmentation để tạo them data training bằng cách translations, horizontal reflections.\n",
    "- **ZFnet(2013)**\n",
    "    - Tương tự AlexNet nhưng có một số điều chỉnh nhỏ.\n",
    "    - Alexnet training trên 15m image trong khi ZF training chỉ có 1.3m image.\n",
    "    - Sử dụng kernel 7x7 ở first layer (alexnet 11x11).Lý do là sử dụng kernel nhỏ hơn để giữ lại nhiều thông tin trên image hơn.\n",
    "    - Tăng số lượng filter nhiều hơn so với alexnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **VGGnet(2014)**\n",
    "    - Xài nhiều Conv nhỏ (3x3, 5x5)\n",
    "    - VGG thì sử dụng các **chuỗi Conv liên tiếp** Conv-Conv-Conv ở middle và end của architect VGG. Việc này sẽ làm cho việc tính toán trở nên lâu hơn nhưng những feature sẽ vẫn được giữ lại nhiều hơn so với việc sử dụng maxpooling sau mỗi Conv (được hỗ trợ bởi GPU)\n",
    "    - Sử dụng relu trong conv\n",
    "    - Architect của VGG16 bao gồm 16 layer: 13 layer Conv (2 layer conv-conv,3 layer conv-conv-conv) đều có kernel 3x3, sau mỗi layer conv là maxpooling (2x2) downsize xuống 0.5, và 3 layer fully connection. \n",
    "    - VGG19 tương tự như VGG16 nhưng có thêm 3 layer conv-conv-conv ở cuối thành chuỗi 4 conv stack/dính với nhau.\n",
    "    - VGG gốc là ở Caffe, được chuyển sang Tensorflow và Keras \n",
    "    - (16layers 135M parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googlenet-Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GoogleNet(2014) - Inception module NN**\n",
    "    - nhiều layers mà it parameter hơn VGG (22layer 5M parameters)\n",
    "    - chỉ có 5m tham số so với alexnet là 60m nhanh hơn gấp 12 lần \n",
    "    - ý tưởng của *Inception module*, nó tính toán các kernel size khác nhau, và pooling từ một input sau đó concatenate nó lại thành output. \n",
    "    - build thêm 2 classifier (2 nhánh phụ) để giảm vanshing gradient. Có hay không 2 nhánh phụ thì thời gian predict cũng không thay đổi nhiều\n",
    "        - loss sẽ tổng hợp của 3 nhánh (sẽ cố định hệ số cho từng nhánh, tông lại không bằng 1)\n",
    "    - Details\n",
    "       - Trong **inception người ta dùng Conv kernel 1x1** với 2 mục đích là giảm tham số tính toán và dimensionality reduction . Dimensionality reduction có thể hiểu làm giảm depth của input (vd iput 28x28x100 qua kernel 1x1 với filter = 10 sẽ giảm depth về còn 28x28x10)\n",
    "           - Conv 1x1 giảm parameter, chạy nhanh hơn\n",
    "           - sau khi qua Conv 1X1 giảm chiều (dim reduction) -> Có thử nhiều kernel size khác sau đó (3x3,5x5,...)  \n",
    "       - Inception v1: có 2 dạng là naïve và dimension reduction. Khác biệt chính đó là version dimension reduction nó dùng conv 1x1 ở mỗi layer để giảm depth của input giúp model có ít tham số hơn. Inception naïve có architect gồm 1x1 conv,3x3 conv, 5x5 conv và 3x3 maxpooling. \n",
    "       - Inception v2 : Cải thiện version 1, thêm layer batchnormalize và giảm Internal Covariate Shift. Ouput của mỗi layer sẽ được normalize về Gaussian N(0,1). Conv 5x5 sẽ được thay thế bằng 2 conv 3x3 để giảm computation cost. \n",
    "       - Inception v3: Điểm đáng chú ý ở version này là Factorization (NxN -> Nx1 * 1xN). Conv 7x7 sẽ được giảm về conv 1 dimesion là (1x7),(7x1). Tương tự conv 3x3 (3x1,1x3). Tăng tốc độ tính toán. Khi tách ra 2 conv thì làm model deeper hơn. \n",
    "           - Nhưng không phải matrix Conv nào cũng factorization được, nên -> Separable Depthwise Conv (MobileNet)\n",
    "       - Inception v4 : là sự kết hợp inception và resnet. Có residual block\n",
    "---\n",
    "- GoogleNet gồm 22 layer, khởi đầu vẫn là những simple convolution layer, tiếp theo là những block của inception module với maxpooling theo sau mỗi block. Một số đặc điểm chính.\n",
    "    - Sử dụng 9 Inception module trên toàn bộ architect. Làm model deeper hơn rất nhiều.\n",
    "    - Không sử dụng fully connection layer mà thay vào đó là **average pooling** từ 7x7x1024 volume thành 1x1x1024 volume giảm thiểu được rất nhiều parameter.\n",
    "    - Inception-ResNet v1 has a computational cost that is similar to that of Inception v3.\n",
    "    - Inception-ResNet v2 has a computational cost that is similar to that of Inception v4.\n",
    "    \n",
    "---    \n",
    "RestNet vs Inception\n",
    "- restnet chưa có optimization\n",
    "    - quantization\n",
    "    - charity sharing\n",
    "- Inception là RF_based NAS , đã được optimize\n",
    "    - nên khi quantization sẽ không tốt bằng\n",
    "\n",
    "**Đọc thêm**\n",
    "- https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n",
    "- https://towardsdatascience.com/deep-learning-understand-the-inception-module-56146866e652"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAS (2017)\n",
    "\n",
    "Sinh ra 1 method NAS - network architech search:\n",
    "- tìm kiếm mạng tối ưu\n",
    "- có thể optimization trên 1 bài toán\n",
    "- nhưng chỉ đúng trên 1 bài toán đó\n",
    "\n",
    "NAS:\n",
    "- RF NAS -> không cân data trước khi train \n",
    "    - tìm được tối ưu\n",
    "    - chạy rất lâu/ high comflexity\n",
    "- Gradient NAS \n",
    "    - phải tự định nghĩa 1 không gian có các block CNN, LSTM, mạng NN nhỏ... (có mấy chục cái thôi)\n",
    "    - thành 1 bài toán classification với lớp cuối cùng là softmax -> cho mỗi block -> nhiều bài toán classification\n",
    "    - giới hạn không gian tìm kiếm, nên nhanh hơn   \n",
    "- Convoluation NAS\n",
    "    - natural selection(genetic algorithm), build population của các block CNN, LSTM, mạng NN nhỏ... (có mấy chục cái thôi)\n",
    "    - có 2 nhánh cross_over (kết nối ngẫu nhiên với các block khác population) và mutation đột biến (bốc ngẫu nhiên và thay đổi)\n",
    "    - cứ tiếp tục loops đến khi tối ưu\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNets(2015)**    \n",
    "- Để hiểu ResNet chúng ta cần hiểu vấn đề khi stack nhiều layer khi training. \n",
    "    - Vấn đề đầu tiên khi tăng model deeper hơn gradient sẽ bị vanishing/explodes. Vấn đề này có thể giải quyết bằng cách thêm Batch Normalization nó giúp normalize output giúp các hệ số trở nên cân bằng hơn không quá nhỏ hoặc quá lớn nên sẽ giúp model dễ hội tụ hơn. \n",
    "    - Vấn đề thứ 2 là degradation, Khi model deeper accuracy bắt đầu bão hòa(saturated) thậm chí là giảm. Resnet được ra đời để giải quyết vấn đề degradation này. \n",
    "- ResNet có architecture gồm nhiều residual layers/block (skip-layer), ý tưởng chính là skip layer bằng cách add connection với layer trước. \n",
    "    - Ý tưởng của residual block là feed foword x(input) qua một số layer conv-max-conv, ta thu được F(x) sau đó add thêm x vào H(x) = F(x) + x . Model sẽ dễ học hơn khi chúng ta thêm feature từ layer trước vào. \n",
    "    \n",
    "    \n",
    "ResNet version:\n",
    "- Each ResNet block is either 2 layer deep (Used in small networks like ResNet 18, 34) or 3 layer deep( ResNet 50, 101, 152).\n",
    "- The Bottleneck class implements a 3 layer block and Basicblock implements a 2 layer block. It also has implementations of all ResNet Architectures with pretrained weights trained on ImageNet.\n",
    "- ResNet được trích dẫn nhiều nhất"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ques**\n",
    "- có transfer learning nhiều nhất\n",
    "- NN rất sâu\n",
    "- xài skip connection -> có những connect sử dụng input nhảy cóc/ không phải trải qua những middle_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đọc thêm**\n",
    "- https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "- deeper - https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Densenet(2016)**     \n",
    "    - skip_connection ở tất cả các điểm/block, ý tưởng cải thiện từ ResNet\n",
    "    - Gần giống Resnet nhưng có một vài điểm khác biệt. Densenet có cấu trúc gồm các dense block và các transition layers. \n",
    "    - Được stack như (dense block) - (transition layers) - (dense block) - (transition layers) \n",
    "    - Ở mỗi dense block sẽ có normalization, nonlinearity và dropout. Để giảm size và depth của feature thì transition layer được đặt giữa các dense block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobilenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sparable Depthwise Convolution\n",
    "Tách 1 block CNN $(K*K*M)$ thành 2 block:\n",
    "     - Depth-wise Conv $(K*K*1)$ \n",
    "     - tiếp tục qua Point-wise Conv $(1*1*M)$ chính là Conv 1x1\n",
    "\n",
    "Cons:\n",
    "- Giảm mạnh số lượng parameters\n",
    "\n",
    "Pros:\n",
    "- Qua depth-wise thông tin giữa các mối liên hệ giữa các chanel -> thay đổi tăng h của Depth-wise -> GROUP CONV BLOCK $(K*K*h)$ h>1, h = 2,3\n",
    "    - GROUP CONV BLOCK nhóm từ trên xuống dưới, không cần overlap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In depthwise convolution [2,3,4], convolution is performed independently for each of input channels. It can also be defined as a special case of grouped convolution where the numbers of input and output channels are same and G equals the number of channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mobilenetv1 (2017):\n",
    "    - Sử dụng cách tính depthwise seperate conv để giảm số lưởng parameter và computational cost\n",
    "    - Nhưng vẫn giữ được độ sâu của NN\n",
    "    - Gồm có 30 layer\n",
    "- Mobilenetv2\n",
    "    - MobileNet V2 still uses depthwise separable convolutions, but its main building block is \n",
    "    -  \n",
    "    - \n",
    "    \n",
    "resources:\n",
    "- [MobileNet: Paper Review and Model Architecture](https://medium.com/@rockyxu399/mobilenet-paper-review-and-model-architecture-7963c22ea528)\n",
    "- [Transfer Learning using Mobilenet keras](https://towardsdatascience.com/transfer-learning-using-mobilenet-and-keras-c75daf7ff299)\n",
    "- [Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast](https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d)\n",
    "- [Giải thích mobilenetv1 và ứng trụng trên ios](https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)\n",
    "- [Review: MobileNetV1 — Depthwise Separable Convolution (Light Weight Model)](https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69)\n",
    "- [MobileNet version 2](https://machinethink.net/blog/mobilenet-v2/)\n",
    "- [Tensorflow pretrained mobilenet](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md?source=post_page-----d9d21774cdab----------------------)\n",
    "- [Sử dụng MobileNet trong C# với EmguCV ](https://forum.machinelearningcoban.com/t/su-dung-mobilenet-trong-c-voi-emgucv/3837)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# CNN - Advanced Cấu Trúc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions CNN - Object Detection (2014)\n",
    "- Là cấu trúc khời đầu cho Object Detection để tìm ra các box của vật thể\n",
    "    - Xem thêm reface-detection để biết ứng dụng trong nhận dạng khuông mặt\n",
    "- Cần sử dụng 1 số search algorithm\n",
    "    - [Selective search](#Selective-Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ý tưởng thuật toán R-CNN khá đơn giản\n",
    "- Bước 1: Dùng Selective Search algorithm để lấy ra khoảng 2000 bounding box trong input mà có khả năng chứa đối tượng.\n",
    "- Bước 2: Cần thêm 1 lớp background (không chứa đối tượng nào), để tách background ra khỏi ds box, giảm số lượng box\n",
    "- Bước 3.1: Sau đó các region proposal được resize lại về cùng kích thước và thực hiện transfer learning với feature extractor, sau đó các extracted feature được cho vào thuật toán SVM để phân loại ảnh. \n",
    "- Bước 3.2: Bên cạnh đó thì extracted feature cũng được dùng để dự đoán 4 offset values cho mỗi cạnh. Ví dụ như khi region proposal chứa người nhưng chỉ có phần thân và nửa mặt, nửa mặt còn lại không có trong region proposal đó thì offset value có thể giúp mở rộng region proposal để lấy được toàn bộ người."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/rcnn1.png\" width=600 height =600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vấn đề với R-CNN\n",
    "Lúc mới xuất hiện thì thuật toán hoạt động khá tốt cho với các thuật toán về computer vision trước đó nhờ vào CNN, tuy nhiên nó vẫn có khá nhiều hạn chế:\n",
    "- Vì với mỗi ảnh ta cần phân loại các class cho 2000 region proposal nên thời gian train rất lâu.\n",
    "- Không thể áp dụng cho real-time thì mỗi ảnh trong test set mất tới 47s để xử lý.\n",
    "- Hạn chế khi chuyển đổi về mobile version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast RCNN (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng Region of Interest (RoI) Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/rcnn2.png\" width=600 height =600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Roi Pooling có cách chia nhỏ để thực hiện max pooling đặc biệt, có thể áp dụng với mọi region matrix, chính là đầu ra của conv5\n",
    "- [giải thích thêm](https://deepsense.ai/region-of-interest-pooling-explained/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster RCNN (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/rcnn3.png\" width=600 height =600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Faster R-CNN giải quyết được vấn đề của Fast R-CNN bằng cách thay thế Selective Search với RPN (Region Proposal Network)\n",
    "- Sử dụng Anchor box, để định vị tâm được cẩn thận hơn\n",
    "\n",
    "Resource:\n",
    "- [giải thích](https://nttuan8.com/bai-11-object-detection-voi-faster-r-cnn/)\n",
    "- [Region Proposal Network (RPN) — Backbone of Faster R-CNN](https://medium.com/egen/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- https://nttuan8.com/bai-11-object-detection-voi-faster-r-cnn/\n",
    "- https://deepmlml.com/rpn-explained.html\n",
    "- https://deepmlml.com/rpn-explained-code-pytorch.html\n",
    "- https://viblo.asia/p/faster-rcnn-for-object-detection-with-keras-aWj53Nwel6m\n",
    "- [Faster R-CNN (object detection) implemented by Keras for custom data from Google’s Open Images Dataset V4](https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## YOLO\n",
    "- Các mô hình R-CNN nói chung có thể chính xác hơn, tuy nhiên họ mô hình YOLO nhanh hơn rất rất nhiều so với R-CNN, và thậm chí đạt được việc phát hiện đối tượng trong thời gian thực."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- [pdkhanh giải thích về Yolo và RCNN](https://phamdinhkhanh.github.io/2019/09/29/OverviewObjectDetection.html)\n",
    "- [pbquoc giải thích Yolo](https://pbcquoc.github.io/yolo/)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/yolo1.png\" width=600 height =600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bước xử lý trong mô hình YOLO (hình ảnh trích xuất từ bài báo gốc). \n",
    "- Đầu tiên mô hình chia hình ảnh thành một grid search kích thước S×S. \n",
    "- Trên mỗi một grid cell ta dự báo một số lượng B bounding boxes và confidence cho những boxes này và phân phối xác suất của C classes. \n",
    "- Như vậy output các dự báo là một tensor kích thước S×S×(B×5+C), giá trị 5 là các tham số của offsets của bounding box gồm x,y,w,h và confidence, C là số lượng tham số của phân phối xác suất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv2 (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLOv2, một instance của mô hình theo như mô tả đã được đào tạo trên hai bộ dữ liệu nhận dạng đối tượng, và có khả năng dự đoán lên tới 9000 loại đối tượng khác nhau, do đó được đặt tên là YOLO9000. \n",
    "- Giống như Faster R-CNN, mô hình YOLOv2 sử dụng anchor boxes, bounding box được xác định trước với hình dạng và kích thước hợp lý được tùy chỉnh trong quá trình huấn luyện. \n",
    "- Sự lựa chọn các bounding boxes cho hình ảnh được xử lý trước bằng cách sử dụng thuật toán phân cụm k-mean trên tập dữ liệu huấn luyện.\n",
    "\n",
    "Điều quan trọng:\n",
    "- Các predicted bounding box được tinh chỉnh để cho phép các thay đổi nhỏ có tác động ít hơn đến các dự đoán, dẫn đến mô hình ổn định hơn. \n",
    "- Thay vì dự đoán trực tiếp vị trí và kích thước, các offsets (tức tọa độ tâm, chiều dài và chiều rộng) được dự đoán để di chuyển và định hình lại các pre-defined anchor boxes tại mỗi một grid cell thông qua hàm logistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ntthanh code lại yolov2](https://trungthanhnguyen0502.github.io/computer%20vision/2018/12/11/yolo_tutorial-3-c%C3%B9ng-code-l%E1%BA%A1i-yolo2/), [1](https://forum.machinelearningcoban.com/t/yolo-tutorial-cung-code-lai-yolo2/3859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv3 (2018)\n",
    "- Có những thay đổi ở mạng CNN trích suất features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/ssd1.jpeg\" width=600 height =600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Cách tính default multi-box cho ssd](https://forum.machinelearningcoban.com/t/cach-tinh-multibox-trong-ssd/3424/4)\n",
    "\n",
    "Resources:\n",
    "- https://phamdinhkhanh.github.io/2019/10/05/SSDModelObjectDetection.html\n",
    "- https://viblo.asia/p/tim-hieu-ve-ssd-multibox-real-time-object-detection-3P0lPEPG5ox\n",
    "- [Understand Single Shot MultiBox Detector (SSD) and Implement It in Pytorch](https://medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad)\n",
    "- [Detect custom object với Yolov3](https://medium.com/sota-tek-jsc/cnn-detect-custom-object-v%E1%BB%9Bi-yolov3-63866906b5dd)\n",
    "- [Giai thich them](https://forum.machinelearningcoban.com/t/object-detection-yolo/503)\n",
    "- [Tải code mẫu](https://forum.machinelearningcoban.com/t/tai-code-mau-object-detection-yolov3-c/6289)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others\n",
    "## RetinaNet\n",
    "- [RetinaNet](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4)\n",
    "## U-Net\n",
    "- [Image segmentation với U-Net](https://nttuan8.com/bai-12-image-segmentation-voi-u-net/)\n",
    "\n",
    "## Efficient Net\n",
    "- https://arxiv.org/abs/1905.11946v1\n",
    "\n",
    "## MXNet\n",
    "- [Implementation of some interesting ideas of deeplearning with MXNet](https://github.com/Ldpe2G/DeepLearningForFun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource\n",
    "- https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
