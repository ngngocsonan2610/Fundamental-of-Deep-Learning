{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Neural Network\n",
    "Table of contents:\n",
    "- [VGG](#VGG)\n",
    "- [Googlenet-Inception](#Googlenet-Inception)\n",
    "- [Resnet](#ResNet)\n",
    "- [Densenet](#Densenet)\n",
    "- [Mobilenet](#Mobilenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "- [Các cấu trúc phổ biến của mạng CNN](https://forum.machinelearningcoban.com/t/kien-truc-cac-mang-cnn-noi-tieng-phan-1-alex-lenet-inception-vgg/2582)\n",
    "- [Qua trinh phat trien NN](https://dlapplications.github.io/2018-07-06-CNN/)\n",
    "- [more comparison explain 1](https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "- [more comparison explain 2](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e4b1)\n",
    "- [more comparison explain 3](https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CNN\"></a>\n",
    "## Conv Layer\n",
    "\n",
    "- [understand](https://topdev.vn/blog/thuat-toan-cnn-convolutional-neural-network/)\n",
    "- https://nttuan8.com/bai-6-convolutional-neural-network/\n",
    "\n",
    "## Tính para\n",
    "<img src=\"image/conv1.webp\" width=400 height = 400>\n",
    "\n",
    "Lưu ý:\n",
    "- Output của convolutional layer sẽ qua hàm activation function trước khi trở thành input của convolutional layer tiếp theo.\n",
    "- Tổng số parameter của layer: Mỗi kernel có kích thước F*F*D và có 1 hệ số bias, nên tổng parameter của 1 kernel là F * F * D + 1. Mà convolutional layer áp dụng K kernel => Tổng số parameter trong layer này là K * (F * F * D + 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/cnn1.jpg\" width=400 height = 400>\n",
    "\n",
    "Convolutional neural network là một mạng neural được ứng dụng rất nhiều trong deep learning trong computer vision cho classifier và localizer . \n",
    "Từ mạng CNN cơ bản người ta có thể tạo ra rất nhiều architect khác nhau, từ những mạng neural cơ bản 1 đến 2 layer đến 100 layer. \n",
    "Đã bao giờ bạn tự hỏi:\n",
    "   - nên sử dụng bao nhiêu layer, nên kết hợp conv với maxpooling thế nào? \n",
    "   - conv-maxpooling hay conv-conv-maxplooling ? \n",
    "   - hay nên sử dụng kernel 3x3 hay 5x5 thậm chí 7x7 điểm khác biệt là gì ? \n",
    "   - Làm gì khi model bị vanishing/exploding gradient, hay tại sao thi thêm nhiều layer hơn thì theo lý thuyết accuarcy phải cao hơn so với shallow model, nhưng thực tế lại không phải accuarcy không tăng thậm chí là giảm đó có phải nguyên nhân do overfitting.\n",
    "Việc tìm hiểu các architure nổi tiếng để xem cấu trúc của nó như thế nào, các ý tưởng về CNN mới nhất hiện nay, từ đó ta có thể trả lời được mấy câu hỏi trên. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://dlapplications.github.io/2018-07-06-CNN/\" width=\"1000\" height=\"600\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://dlapplications.github.io/2018-07-06-CNN/\" width=\"1000\" height=\"600\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LeNet(1998)** \n",
    "    - Cấu trúc của LeNet gồm 2 layer (Convolution + maxpooling) và 2 layer fully connected layer và output là softmax layer \n",
    "    - Softmax layer, output = 10 (train với mnist nên 10 digits). Nhược điểm của LeNet là mạng còn rất đơn giản và sử dụng sigmoid (or tanh) ở mỗi convolution layer mạng tính toán rất chậm.\n",
    "- **Alexnet(2012)**\n",
    "    - Sử dụng relu thay cho sigmoid(or tanh) để xử lý với non-linearity. Tăng tốc độ tính toán lên 6 lần.\n",
    "    - Sử dụng dropout như một phương pháp regularization mới cho CNN. Dropout không những giúp mô hình tránh được overfitting mà còn làm giảm thời gian huấn luyện mô hình\n",
    "    - Overlap pooling để giảm size của network ( Traditionally pooling regions không overlap).\n",
    "    - Sử dụng local response normalization để chuẩn hóa ở mỗi layer.\n",
    "    - Sử dụng kỹ thuật data augmentation để tạo them data training bằng cách translations, horizontal reflections.\n",
    "- **ZFnet(2013)**\n",
    "    - Tương tự AlexNet nhưng có một số điều chỉnh nhỏ.\n",
    "    - Alexnet training trên 15m image trong khi ZF training chỉ có 1.3m image.\n",
    "    - Sử dụng kernel 7x7 ở first layer (alexnet 11x11).Lý do là sử dụng kernel nhỏ hơn để giữ lại nhiều thông tin trên image hơn.\n",
    "    - Tăng số lượng filter nhiều hơn so với alexnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **VGGnet(2014)**\n",
    "    - VGG thì sử dụng các **chuỗi Conv liên tiếp** Conv-Conv-Conv ở middle và end của architect VGG. Việc này sẽ làm cho việc tính toán trở nên lâu hơn nhưng những feature sẽ vẫn được giữ lại nhiều hơn so với việc sử dụng maxpooling sau mỗi Conv (được hỗ trợ bởi GPU)\n",
    "    - Sử dụng relu trong conv\n",
    "    - Architect của VGG16 bao gồm 16 layer: 13 layer Conv (2 layer conv-conv,3 layer conv-conv-conv) đều có kernel 3x3, sau mỗi layer conv là maxpooling (2x2) downsize xuống 0.5, và 3 layer fully connection. \n",
    "    - VGG19 tương tự như VGG16 nhưng có thêm 3 layer conv-conv-conv ở cuối thành chuỗi 4 conv stack/dính với nhau.\n",
    "    - VGG gốc là ở Caffe, được chuyển sang Tensorflow và Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Googlenet-Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GoogleNet(2014) - Inception module NN**\n",
    "    - chỉ có 5m tham số so với alexnet là 60m nhanh hơn gấp 12 lần \n",
    "    - ý tưởng của *Inception module*, nó tính toán các kernel size khác nhau, và pooling từ một input sau đó concatenate nó lại thành output. \n",
    "    - <img src=\"image/cnn5.jpg\" width=400 height = 400> :\n",
    "       - Trong **inception người ta dùng conv kernel 1x1** với 2 mục đích là giảm tham số tính toán và dimensionality reduction . Dimensionality reduction có thể hiểu làm giảm depth của input (vd iput 28x28x100 qua kernel 1x1 với filter = 10 sẽ giảm depth về còn 28x28x10)\n",
    "       - Inception v1: có 2 dạng là naïve và dimension reduction. Khác biệt chính đó là version dimension reduction nó dùng conv 1x1 ở mỗi layer để giảm depth của input giúp model có ít tham số hơn. Inception naïve có architect gồm 1x1 conv,3x3 conv, 5x5 conv và 3x3 maxpooling. \n",
    "       - Inception v2 : Cải thiện version 1, thêm layer batchnormalize và giảm Internal Covariate Shift. Ouput của mỗi layer sẽ được normalize về Gaussian N(0,1). Conv 5x5 sẽ được thay thế bằng 2 conv 3x3 để giảm computation cost. \n",
    "       - Inception v3: Điểm đáng chú ý ở version này là Factorization. Conv 7x7 sẽ được giảm về conv 1 dimesion là (1x7),(7x1). Tương tự conv 3x3 (3x1,1x3). Tăng tốc độ tính toán. Khi tách ra 2 conv thì làm model deeper hơn. \n",
    "       - Inception v4 : là sự kết hợp inception và resnet. Có residual block\n",
    "    - GoogleNet gồm 22 layer, khởi đầu vẫn là những simple convolution layer, tiếp theo là những block của inception module với maxpooling theo sau mỗi block. Một số đặc điểm chính.\n",
    "        - Sử dụng 9 Inception module trên toàn bộ architect. Làm model deeper hơn rất nhiều.\n",
    "        - Không sử dụng fully connection layer mà thay vào đó là **average pooling** từ 7x7x1024 volume thành 1x1x1024 volume giảm thiểu được rất nhiều parameter.\n",
    "        - Inception-ResNet v1 has a computational cost that is similar to that of Inception v3.\n",
    "        - Inception-ResNet v2 has a computational cost that is similar to that of Inception v4.\n",
    "        \n",
    "**Đọc thêm**\n",
    "- https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ResNets(2015)**    \n",
    "    - Để hiểu ResNet chúng ta cần hiểu vấn đề khi stack nhiều layer khi training. \n",
    "        - Vấn đề đầu tiên khi tăng model deeper hơn gradient sẽ bị vanishing/explodes. Vấn đề này có thể giải quyết bằng cách thêm Batch Normalization nó giúp normalize output giúp các hệ số trở nên cân bằng hơn không quá nhỏ hoặc quá lớn nên sẽ giúp model dễ hội tụ hơn. \n",
    "        - Vấn đề thứ 2 là degradation, Khi model deeper accuracy bắt đầu bão hòa(saturated) thậm chí là giảm. Resnet được ra đời để giải quyết vấn đề degradation này. \n",
    "    - ResNet có architecture gồm nhiều residual block, ý tưởng chính là skip layer bằng cách add connection với layer trước. \n",
    "        - Ý tưởng của residual block là feed foword x(input) qua một số layer conv-max-conv, ta thu được F(x) sau đó add thêm x vào H(x) = F(x) + x . Model sẽ dễ học hơn khi chúng ta thêm feature từ layer trước vào. \n",
    "        - <img src=\"image/cnn12.jpg\" width=400 height = 400>   \n",
    "    - ResNet version\n",
    "        - <img src=\"image/resnet1.png\" width=600 height = 600>  \n",
    "        - Each ResNet block is either 2 layer deep (Used in small networks like ResNet 18, 34) or 3 layer deep( ResNet 50, 101, 152).\n",
    "        - <img src=\"image/resnet2.png\" width=400 height = 400>  \n",
    "        - The Bottleneck class implements a 3 layer block and Basicblock implements a 2 layer block. It also has implementations of all ResNet Architectures with pretrained weights trained on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đọc thêm**\n",
    "- https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "- deeper - https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Densenet(2016)**     \n",
    "    - Gần giống Resnet nhưng có một vài điểm khác biệt. Densenet có cấu trúc gồm các dense block và các transition layers. \n",
    "    - Được stack như (dense block) - (transition layers) - (dense block) - (transition layers) \n",
    "    - Ở mỗi dense block sẽ có normalization, nonlinearity và dropout. Để giảm size và depth của feature thì transition layer được đặt giữa các dense block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/mobilenet1.png\" width=600 height = 600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depthwise Convolution\n",
    "\n",
    "In depthwise convolution [2,3,4], convolution is performed independently for each of input channels. It can also be defined as a special case of grouped convolution where the numbers of input and output channels are same and G equals the number of channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mobilenetv1 (2017):\n",
    "    - Sử dụng cách tính depthwise seperate conv để giảm số lưởng parameter và computational cost\n",
    "    - Nhưng vẫn giữ được độ sâu của NN\n",
    "    - Gồm có 30 layer\n",
    "- Mobilenetv2\n",
    "    - MobileNet V2 still uses depthwise separable convolutions, but its main building block is \n",
    "    - <img src=\"image/mobilenet2.png\" width=300 height = 300>  \n",
    "    - \n",
    "    \n",
    "resources:\n",
    "- [MobileNet: Paper Review and Model Architecture](https://medium.com/@rockyxu399/mobilenet-paper-review-and-model-architecture-7963c22ea528)\n",
    "- [Transfer Learning using Mobilenet keras](https://towardsdatascience.com/transfer-learning-using-mobilenet-and-keras-c75daf7ff299)\n",
    "- [Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast](https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d)\n",
    "- [Giải thích mobilenetv1 và ứng trụng trên ios](https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)\n",
    "- [Review: MobileNetV1 — Depthwise Separable Convolution (Light Weight Model)](https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69)\n",
    "- [MobileNet version 2](https://machinethink.net/blog/mobilenet-v2/)\n",
    "- [Tensorflow pretrained mobilenet](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md?source=post_page-----d9d21774cdab----------------------)\n",
    "- [Sử dụng MobileNet trong C# với EmguCV ](https://forum.machinelearningcoban.com/t/su-dung-mobilenet-trong-c-voi-emgucv/3837)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Yolo](https://forum.machinelearningcoban.com/t/yolo-tutorial-cung-code-lai-yolo2/3859)\n",
    "- [RetinaNet](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
