{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBVQntKmkBDw"
   },
   "source": [
    "# **Tutourial**\n",
    "\n",
    "\n",
    "DL : ly thuyet va code\n",
    "- https://github.com/mbadry1/DeepLearning.ai-Summary\n",
    "- [note-deeplearningai](https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/)\n",
    "- [cheat-sheat](https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#lstm)\n",
    "\n",
    "Blog:\n",
    "- [MIT-Lex Fridman](https://medium.com/tensorflow/mit-deep-learning-basics-introduction-and-overview-with-tensorflow-355bcd26baf0)\n",
    "- [ongxuanhong-DL](https://ongxuanhong.wordpress.com/category/data-science/deep-learning/)\n",
    "- [https://dlapplications.github.io/]\n",
    "- [https://nttuan8.com/]\n",
    "- [https://aivietnam.ai/courses/aisummer2019/lessons/gioi-thieu-ve-khoa-hoc/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRSO8KKkHbxA"
   },
   "source": [
    "# Table of content\n",
    "1. [Multi-Perceptron](#multi-perceptron)\n",
    "2. [CNN](#CNN)\n",
    "\n",
    "- [Transfer Learning](#transferlearning)\n",
    "- [Validation of Neural Network](#validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dVNXFhf2_B7"
   },
   "source": [
    "<a id=\"multi-perceptron\"></a>\n",
    "# 1. Multi-Perceptron\n",
    "\n",
    "Resources:\n",
    "[Ý tưởng Softmax Regression, loss function: cross-entropy](https://machinelearningcoban.com/2017/02/17/softmax/#-cross-entropy)\n",
    ">```Softmax Regression đặc biệt được sử dụng nhiều trong các mạng Neural có nhiều lớp (Deep Neural Networks hay DNN). Những lớp phía trước có thể được coi như một bộ Feature Extractor, lớp cuối cùng của DNN cho bài toán classification thường là Softmax Regression.```\n",
    "\n",
    "- [Lược sử ý tưởng về Deep Learning](https://dlapplications.github.io/2018-06-27-Brief-History-of-Deep-learning/)\n",
    "- [Perceptron](https://dlapplications.github.io/2018-06-11-perceptron/)\n",
    "- [Multi-perceptron](https://dlapplications.github.io/2018-06-15-MLP/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFUr3XT-2-4E"
   },
   "source": [
    "## Giải thích Feedforward & Backpropagation \n",
    "\n",
    "Sources:\n",
    "\n",
    "   - [Multi-layer Perceptron và Backpropagation - mlcoban](https://machinelearningcoban.com/2017/02/24/mlp/)\n",
    "   - Đi từ [linear regression](https://nttuan8.com/bai-1-linear-regression-va-gradient-descent/#Gaussian_Process_(GP)),[Logistic regression](https://nttuan8.com/bai-2-logistic-regression/) đến [simple NN](https://nttuan8.com/bai-3-neural-network/) và [Feedforward & Backpropagation](https://nttuan8.com/bai-4-backpropagation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1bwPIlsJDvE"
   },
   "source": [
    "[Xem them giai thich toan&code .Mathematic-NN](Mathematic-NN/CNN-math.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vljP43l2tsWh",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Layers\n",
    "\n",
    "Source \n",
    "- [tf-layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "- [keras-layer explain viblo](https://viblo.asia/p/deep-learning-qua-kho-dung-lo-da-co-keras-part-2-LzD5dbPzZjY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "- còn gọi là `dense layer` trong keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhes7yDS-F5H"
   },
   "source": [
    "<a id=\"CNN\"></a>\n",
    "## Conv Layer\n",
    "\n",
    "- [understand](https://topdev.vn/blog/thuat-toan-cnn-convolutional-neural-network/)\n",
    "- [https://nttuan8.com/bai-6-convolutional-neural-network/]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrrSMqQT2_Ed"
   },
   "source": [
    "## Active function\n",
    "- [ML-Regression](https://colab.research.google.com/drive/15RAbgNf-2Lkv_TOjgYtDafWvy3vASbzy#scrollTo=IDHLmr9DPmak)\n",
    "- https://aivietnam.ai/courses/aisummer2019/lessons/ham-activations/\n",
    "- [Tf-keras activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations), [keras](https://keras.io/activations/), [more-explain](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjNhjU1IAp7V"
   },
   "source": [
    "**Choosing the right Activation Function**\n",
    "\n",
    "Now that we have seen so many activation  functions, we need some logic / heuristics to know which activation function should be used in which situation. Good or bad – there is no rule of thumb.\n",
    "\n",
    "However depending upon the properties of the problem we might be able to make a better choice for easy and quicker convergence of the network.\n",
    "\n",
    "*  **Sigmoid** functions and their combinations generally work better in the case of classifiers\n",
    "*  **Sigmoids and tanh** functions are sometimes avoided due to the vanishing gradient problem\n",
    "*  **ReLU** function is a general activation function and is used in most cases these days\n",
    "*  If we encounter a case of dead neurons in our networks the leaky **ReLU** function is the best choice\n",
    "*  Always keep in mind that **ReLU** function should only be used in the hidden layers\n",
    "*  As a rule of thumb, you can begin with using **ReLU** function and then move over to other activation functions in case **ReLU** doesn’t provide with optimum results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Connected Layer**\n",
    "\n",
    "[understand1](https://pennlio.wordpress.com/2014/04/11/fully-connected-locally-connected-and-shared-weights-layer-in-neural-networks/), [understand-2](https://prateekvjoshi.com/2016/04/12/understanding-locally-connected-layers-in-convolutional-neural-networks/)\n",
    "\n",
    "Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer\n",
    "- Densly\n",
    "- Fully\n",
    "- Locally\n",
    "\n",
    "Source: [keras-connected](https://keras.io/layers/local/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvWgqmoQMGvm"
   },
   "source": [
    "## **Pooling Layer**\n",
    "\n",
    "The Pooling layer is responsible for reducing the spatial size of the Convolved Feature. \n",
    "- This is to decrease the computational power required to process the data through dimensionality reduction. \n",
    "- Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n",
    "\n",
    "Source: [keras-pooling](https://keras.io/layers/pooling/)\n",
    "\n",
    "# **Core layers**\n",
    "\n",
    "- **Dropout**\n",
    "  - [understand](https://www.phamduytung.com/blog/2019-05-05-deep-learning-dropout/)\n",
    "  - [dropout and noise layer](https://medium.com/@maksutov.rn/deep-study-of-a-not-very-deep-neural-network-part-5-dropout-and-noise-29d980ece933)\n",
    "  - có để thực hiện CNN với kernel 1x1x3 -> hoạt động giống relularzization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kEQh8u6p5w2"
   },
   "source": [
    "## **Normalization Layers**\n",
    "\n",
    "[understand](https://forum.machinelearningcoban.com/t/correct-me-if-i-am-wrong-batch-normalize/2561)\n",
    "\n",
    "- sử dụng trong Inception v2 : Cải thiện version 1, thêm layer batchnormalize và giảm Internal Covariate Shift. Ouput của mỗi layer sẽ được normalize về Gaussian N(0,1)\n",
    "\n",
    "Source: [Batch-norm](https://keras.io/layers/normalization/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xNH4-iqpmT4"
   },
   "source": [
    "## **Embedding & Merge Layers**\n",
    "\n",
    "[understand](https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9)\n",
    "Source: [keraas-embed](https://keras.io/layers/embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yY7oooODCt5J"
   },
   "source": [
    "## **Noise Layer**\n",
    "\n",
    "[understand](https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3)\n",
    "\n",
    "source: [keras-noise](https://keras.io/layers/noise/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quá trình sử dụng pre-trained model như trên gọi là transfer learning.\n",
    "Có 2 loại transfer learning:\n",
    "- Feature extractor: Sau khi lấy ra các đặc điểm của ảnh bằng việc sử dụng ConvNet của pre-trained model, thì ta sẽ dùng linear classifier (linear SVM, softmax classifier,..) để phân loại ảnh. Hiểu đơn giản thì các đặc điểm ảnh (tai, mũi, tóc,…) giờ như input của bài toán linear regression hay logistic regression.\n",
    "- Fine tuning: Sau khi lấy ra các đặc điểm của ảnh bằng việc sử dụng ConvNet của pre-trained model, thì ta sẽ coi đây là input của 1 CNN mới bằng cách thêm các ConvNet và Fully Connected layer. Lý do là ConvNet của VGGFace 2 model có thể lấy ra được các thuộc tính của mặt người nói chung nhưng người Việt Nam có nhưng đặc tính khác nên cần thêm 1 số Convnet mới để học thêm các thuộc tính của người Việt Nam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPbU5I5_2-eU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbczoGOz22o7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2.DL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
