{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoEncoder.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Km2CN1Kain6U","colab_type":"text"},"source":["# Guide\n","- [Deep inside: Autoencoders](https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f)\n","- [Autoencoders: Neural Networks for Unsupervised Learning](https://medium.com/intuitive-deep-learning/autoencoders-neural-networks-for-unsupervised-learning-83af5f092f0b)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7AUwFllRdshe","colab_type":"text"},"source":["# Autoencoder (AE) \n","AE are neural networks that aims to copy their inputs to their outputs.\n","\n","- Ý tưởng:\n","  - They work by compressing the input -> into a **laten-space reprentation**\n","  - Then ->, recontructing the output from this presentation.\n","\n","- What are autoencoders used for ?\n","  - data denoising\n","  - dimensionality reduction\n","  - autoencoders will do a poor job for image compression\n"]},{"cell_type":"code","metadata":{"id":"m4bLPNlkdSwu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"464N2lASC-Fi","colab_type":"text"},"source":["# Variational AE\n","Tư tưởng chính của VAE là thay vì tìm một điểm biểu diễn trong không gian ẩn cho một điểm dữ liệu trong không gian gốc, chúng ta sẽ đi tìm một phân phối xác suất cho điểm dữ liệu đó. \n","\n","Loss function của VAE gồm 2 phần:\n","- Reconstruction loss (giống với Autoencoder thông thường)\n","- KL loss mình đã trình bày phía trên\n","\n","\n","Sources:\n","- [Variational Autoencoder (VAE) cơ bản](https://forum.machinelearningcoban.com/t/variational-autoencoder-vae-co-ban/6494)\n","- [read more](https://www.cs.princeton.edu/courses/archive/spring17/cos598E/alex.pdf)"]}]}